{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from collections import OrderedDict\n",
    "from torch import nn\n",
    "from torch import asin\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Download data from FashionMNIST\n",
    "\"\"\"\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import LinearArcsine, RandomFeatureMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "arcsin\n",
    "nn with arcsine activation\n",
    "\"\"\"\n",
    "class ArcsinNN(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, hidden_features: List[int] = None):\n",
    "        \"\"\"\n",
    "        Initialize an ArcsinNN\n",
    "        hidden_features: a list contains the dimension of hidden layers\n",
    "        \"\"\"\n",
    "        super(ArcsinNN, self).__init__()\n",
    "        self.num_hidden_layers = len(hidden_features) if hidden_features else 0\n",
    "        self.Flatten = nn.Flatten() # flatten the input\n",
    "\n",
    "        if self.num_hidden_layers:\n",
    "            Layers = []\n",
    "            dims = [in_features] + hidden_features\n",
    "            for i, (in_dim, out_dim) in enumerate(zip(dims[:-1], dims[1:])):\n",
    "                Layers.append(('LinearArcsine'+f'{i}', LinearArcsine(in_features = in_dim, out_features = out_dim)))\n",
    "            Layers.append(('Output', nn.Linear(in_features = hidden_features[-1], out_features= out_features)))\n",
    "            self.Layers = nn.Sequential(OrderedDict(Layers))\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Missing hidden_feautres!\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "       \n",
    "        x = self.Flatten(x)\n",
    "        logits = self.Layers(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArcsinNN(\n",
      "  (Flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (Layers): Sequential(\n",
      "    (LinearArcsine0): LinearArcsine(in_features=784, out_features=1024, bias=True)\n",
      "    (LinearArcsine1): LinearArcsine(in_features=1024, out_features=1024, bias=True)\n",
      "    (LinearArcsine2): LinearArcsine(in_features=1024, out_features=512, bias=True)\n",
      "    (Output): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ArcsinNN(in_features=28*28, out_features=10, hidden_features=[1024,1024,512]).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ApproxArcsineNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Given a valid ArcsinNN model, approximate the ArcsinNN using RandomFeatureMap for each LinearArcsine layers\n",
    "    \"\"\"\n",
    "    def __init__(self, model: ArcsinNN = None):\n",
    "        super(ApproxArcsineNN, self).__init__()\n",
    "        if model is None or type(model) is not ArcsinNN:\n",
    "            raise ValueError(\"Missing input ArcsinNN model!\")\n",
    "\n",
    "        self.num_hidden_layers = model.num_hidden_layers\n",
    "\n",
    "        self.Flatten = nn.Flatten() # flatten the input\n",
    "        \n",
    "        # create random feature maps\n",
    "        dims = set(model.Layers[i].in_features for i in range(self.num_hidden_layers))\n",
    "        self.RandomFeatureMaps = {d: RandomFeatureMap(d+1, device=model.Layers[0].weight.device).float() for d in dims}\n",
    "\n",
    "        # copy and paste weights\n",
    "        self.Linears = nn.ModuleList([nn.Linear(in_features=model.Layers[i].in_features, out_features=model.Layers[i].out_features) for i in range(self.num_hidden_layers)])\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            self.Linears[i].weight = nn.Parameter(model.Layers[i].weight.clone().detach())\n",
    "            self.Linears[i].bias = nn.Parameter(model.Layers[i].bias.clone().detach())\n",
    "        self.Output = nn.Linear(in_features=model.Layers[-1].in_features, out_features=model.Layers[-1].out_features)\n",
    "        self.Output.weight = nn.Parameter(model.Layers[-1].weight.clone().detach())\n",
    "        self.Output.bias = nn.Parameter(model.Layers[-1].bias.clone().detach())\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.Flatten(x) # [n, D_in]\n",
    "\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            n, D = x.shape[0], x.shape[1]\n",
    "            x = torch.concat([x, torch.ones((n, 1), device=x.device)], dim = 1) # [n, D_in + 1]\n",
    "            W = torch.concat([self.Linears[i].weight, self.Linears[i].bias], dim = 0) # [D_in + 1, D_out]\n",
    "            phi_x = self.RandomFeatureMaps[D](x) # [n, D_in + 1]\n",
    "            phi_W = self.RandomFeatureMaps[D](W.T) # [D_out, D_in + 1]\n",
    "            x = (np.pi/2)*((phi_x @ phi_W.T)/(D+1))\n",
    "\n",
    "        # output layer\n",
    "        logits = self.Output(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepresentArcsineNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Given a valid ArcsinNN model, approximate the ArcsinNN using RandomFeatureMap for each LinearArcsine layers, and represent using composition of feature maps.\n",
    "    \"\"\"\n",
    "    def __init__(self, model: ArcsinNN = None):\n",
    "        super(RepresentArcsineNN, self).__init__()\n",
    "        if model is None or type(model) is not ArcsinNN:\n",
    "            raise ValueError(\"Missing input ArcsinNN model!\")\n",
    "\n",
    "        self.num_hidden_layers = model.num_hidden_layers\n",
    "\n",
    "        self.Flatten = nn.Flatten() # flatten the input\n",
    "        \n",
    "        # create random feature maps\n",
    "        self.input_dim = model.Layers[0].in_features\n",
    "        self.RandomFeatureMaps = {i: RandomFeatureMap(self.input_dim + i + 1, device = model.Layers[0].weight.device).float() for i in range(self.num_hidden_layers)}\n",
    "\n",
    "        # copy and paste weights\n",
    "        self.Linears = nn.ModuleList([nn.Linear(in_features=model.Layers[i].in_features, out_features=model.Layers[i].out_features) for i in range(self.num_hidden_layers)])\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            self.Linears[i].weight = nn.Parameter(model.Layers[i].weight.clone().detach())\n",
    "            self.Linears[i].bias = nn.Parameter(model.Layers[i].bias.clone().detach())\n",
    "        self.Output = nn.Linear(in_features=model.Layers[-1].in_features, out_features=model.Layers[-1].out_features)\n",
    "        self.Output.weight = nn.Parameter(model.Layers[-1].weight.clone().detach())\n",
    "        self.Output.bias = nn.Parameter(model.Layers[-1].bias.clone().detach())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.Flatten(x)\n",
    "        \n",
    "        n, D = x.shape[0], x.shape[1]        \n",
    "        W = torch.eye(self.Linears[0].weight.shape[0], device=self.Linears[0].weight.device)\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            # compute phi(phi(...phi(x))) \n",
    "            x = torch.concat([x, torch.ones((n, 1), device=x.device)], dim = 1)   \n",
    "            x = self.RandomFeatureMaps[i](x)\n",
    "\n",
    "            # compute phi(phi(...phi(W)))\n",
    "            W = torch.matmul(W, self.Linears[i].weight)\n",
    "            W = torch.concat([W, self.Linears[i].bias], dim = 0)\n",
    "            W = (torch.pi/2/W.shape[0]) * self.RandomFeatureMaps[i](W.T).T\n",
    "\n",
    "        x = torch.matmul(x, W)\n",
    "\n",
    "        logits = self.Output(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5*1e-3\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(training_data,batch_size=batch_size,collate_fn=lambda x:tuple(x_.to(device) for x_ in default_collate(x)))\n",
    "test_loader = DataLoader(test_data,batch_size=batch_size,collate_fn=lambda x:tuple(x_.to(device) for x_ in default_collate(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X.float())\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X.float())\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.298501  [    0/60000]\n",
      "loss: 0.608058  [ 6400/60000]\n",
      "loss: 0.428516  [12800/60000]\n",
      "loss: 0.697981  [19200/60000]\n",
      "loss: 0.560684  [25600/60000]\n",
      "loss: 0.475368  [32000/60000]\n",
      "loss: 0.480378  [38400/60000]\n",
      "loss: 0.529320  [44800/60000]\n",
      "loss: 0.520500  [51200/60000]\n",
      "loss: 0.524951  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.7%, Avg loss: 0.553309 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.429452  [    0/60000]\n",
      "loss: 0.458487  [ 6400/60000]\n",
      "loss: 0.385785  [12800/60000]\n",
      "loss: 0.557985  [19200/60000]\n",
      "loss: 0.591877  [25600/60000]\n",
      "loss: 0.452306  [32000/60000]\n",
      "loss: 0.435526  [38400/60000]\n",
      "loss: 0.494277  [44800/60000]\n",
      "loss: 0.525467  [51200/60000]\n",
      "loss: 0.521806  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.6%, Avg loss: 0.520884 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.371641  [    0/60000]\n",
      "loss: 0.450556  [ 6400/60000]\n",
      "loss: 0.336251  [12800/60000]\n",
      "loss: 0.529340  [19200/60000]\n",
      "loss: 0.557300  [25600/60000]\n",
      "loss: 0.435475  [32000/60000]\n",
      "loss: 0.402767  [38400/60000]\n",
      "loss: 0.506616  [44800/60000]\n",
      "loss: 0.523694  [51200/60000]\n",
      "loss: 0.519065  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.478008 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.323761  [    0/60000]\n",
      "loss: 0.455790  [ 6400/60000]\n",
      "loss: 0.316639  [12800/60000]\n",
      "loss: 0.509748  [19200/60000]\n",
      "loss: 0.538731  [25600/60000]\n",
      "loss: 0.417822  [32000/60000]\n",
      "loss: 0.371292  [38400/60000]\n",
      "loss: 0.515693  [44800/60000]\n",
      "loss: 0.506255  [51200/60000]\n",
      "loss: 0.516089  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.462731 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.285020  [    0/60000]\n",
      "loss: 0.438137  [ 6400/60000]\n",
      "loss: 0.304785  [12800/60000]\n",
      "loss: 0.493439  [19200/60000]\n",
      "loss: 0.520060  [25600/60000]\n",
      "loss: 0.390788  [32000/60000]\n",
      "loss: 0.363690  [38400/60000]\n",
      "loss: 0.515367  [44800/60000]\n",
      "loss: 0.487619  [51200/60000]\n",
      "loss: 0.503194  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.460858 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.272998  [    0/60000]\n",
      "loss: 0.442766  [ 6400/60000]\n",
      "loss: 0.295758  [12800/60000]\n",
      "loss: 0.483090  [19200/60000]\n",
      "loss: 0.507289  [25600/60000]\n",
      "loss: 0.364502  [32000/60000]\n",
      "loss: 0.357392  [38400/60000]\n",
      "loss: 0.509149  [44800/60000]\n",
      "loss: 0.478660  [51200/60000]\n",
      "loss: 0.481201  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.462194 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.269644  [    0/60000]\n",
      "loss: 0.430575  [ 6400/60000]\n",
      "loss: 0.290785  [12800/60000]\n",
      "loss: 0.475378  [19200/60000]\n",
      "loss: 0.502419  [25600/60000]\n",
      "loss: 0.343540  [32000/60000]\n",
      "loss: 0.350651  [38400/60000]\n",
      "loss: 0.500631  [44800/60000]\n",
      "loss: 0.475472  [51200/60000]\n",
      "loss: 0.463513  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.462471 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.265366  [    0/60000]\n",
      "loss: 0.417479  [ 6400/60000]\n",
      "loss: 0.288590  [12800/60000]\n",
      "loss: 0.469433  [19200/60000]\n",
      "loss: 0.504051  [25600/60000]\n",
      "loss: 0.327636  [32000/60000]\n",
      "loss: 0.343816  [38400/60000]\n",
      "loss: 0.491636  [44800/60000]\n",
      "loss: 0.473727  [51200/60000]\n",
      "loss: 0.450696  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.4%, Avg loss: 0.461386 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.257602  [    0/60000]\n",
      "loss: 0.405216  [ 6400/60000]\n",
      "loss: 0.287690  [12800/60000]\n",
      "loss: 0.464726  [19200/60000]\n",
      "loss: 0.504634  [25600/60000]\n",
      "loss: 0.317977  [32000/60000]\n",
      "loss: 0.340458  [38400/60000]\n",
      "loss: 0.486943  [44800/60000]\n",
      "loss: 0.473346  [51200/60000]\n",
      "loss: 0.443713  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.5%, Avg loss: 0.459571 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.251262  [    0/60000]\n",
      "loss: 0.396091  [ 6400/60000]\n",
      "loss: 0.287438  [12800/60000]\n",
      "loss: 0.460426  [19200/60000]\n",
      "loss: 0.502110  [25600/60000]\n",
      "loss: 0.312434  [32000/60000]\n",
      "loss: 0.338071  [38400/60000]\n",
      "loss: 0.483945  [44800/60000]\n",
      "loss: 0.473145  [51200/60000]\n",
      "loss: 0.440154  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.6%, Avg loss: 0.457731 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.246833  [    0/60000]\n",
      "loss: 0.391307  [ 6400/60000]\n",
      "loss: 0.287431  [12800/60000]\n",
      "loss: 0.456192  [19200/60000]\n",
      "loss: 0.498776  [25600/60000]\n",
      "loss: 0.308891  [32000/60000]\n",
      "loss: 0.334890  [38400/60000]\n",
      "loss: 0.481387  [44800/60000]\n",
      "loss: 0.473376  [51200/60000]\n",
      "loss: 0.437260  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.8%, Avg loss: 0.455927 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.243397  [    0/60000]\n",
      "loss: 0.388201  [ 6400/60000]\n",
      "loss: 0.287920  [12800/60000]\n",
      "loss: 0.451355  [19200/60000]\n",
      "loss: 0.495201  [25600/60000]\n",
      "loss: 0.306688  [32000/60000]\n",
      "loss: 0.331232  [38400/60000]\n",
      "loss: 0.478941  [44800/60000]\n",
      "loss: 0.473330  [51200/60000]\n",
      "loss: 0.433551  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.454370 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.240698  [    0/60000]\n",
      "loss: 0.385583  [ 6400/60000]\n",
      "loss: 0.289105  [12800/60000]\n",
      "loss: 0.446039  [19200/60000]\n",
      "loss: 0.492069  [25600/60000]\n",
      "loss: 0.305613  [32000/60000]\n",
      "loss: 0.328348  [38400/60000]\n",
      "loss: 0.476984  [44800/60000]\n",
      "loss: 0.473046  [51200/60000]\n",
      "loss: 0.428912  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.452811 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.238074  [    0/60000]\n",
      "loss: 0.383067  [ 6400/60000]\n",
      "loss: 0.290664  [12800/60000]\n",
      "loss: 0.440364  [19200/60000]\n",
      "loss: 0.489697  [25600/60000]\n",
      "loss: 0.305551  [32000/60000]\n",
      "loss: 0.325574  [38400/60000]\n",
      "loss: 0.475485  [44800/60000]\n",
      "loss: 0.472194  [51200/60000]\n",
      "loss: 0.424172  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.0%, Avg loss: 0.451131 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.234788  [    0/60000]\n",
      "loss: 0.380524  [ 6400/60000]\n",
      "loss: 0.292582  [12800/60000]\n",
      "loss: 0.434865  [19200/60000]\n",
      "loss: 0.487949  [25600/60000]\n",
      "loss: 0.304648  [32000/60000]\n",
      "loss: 0.322636  [38400/60000]\n",
      "loss: 0.474181  [44800/60000]\n",
      "loss: 0.471063  [51200/60000]\n",
      "loss: 0.419676  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.1%, Avg loss: 0.449366 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.231140  [    0/60000]\n",
      "loss: 0.377846  [ 6400/60000]\n",
      "loss: 0.294030  [12800/60000]\n",
      "loss: 0.429842  [19200/60000]\n",
      "loss: 0.486013  [25600/60000]\n",
      "loss: 0.303535  [32000/60000]\n",
      "loss: 0.319821  [38400/60000]\n",
      "loss: 0.472836  [44800/60000]\n",
      "loss: 0.469710  [51200/60000]\n",
      "loss: 0.415373  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 0.447454 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.227422  [    0/60000]\n",
      "loss: 0.375365  [ 6400/60000]\n",
      "loss: 0.294831  [12800/60000]\n",
      "loss: 0.424912  [19200/60000]\n",
      "loss: 0.484274  [25600/60000]\n",
      "loss: 0.302401  [32000/60000]\n",
      "loss: 0.317240  [38400/60000]\n",
      "loss: 0.471934  [44800/60000]\n",
      "loss: 0.468318  [51200/60000]\n",
      "loss: 0.411481  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 0.445736 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.224162  [    0/60000]\n",
      "loss: 0.373285  [ 6400/60000]\n",
      "loss: 0.294903  [12800/60000]\n",
      "loss: 0.420510  [19200/60000]\n",
      "loss: 0.482041  [25600/60000]\n",
      "loss: 0.299971  [32000/60000]\n",
      "loss: 0.314883  [38400/60000]\n",
      "loss: 0.471749  [44800/60000]\n",
      "loss: 0.466764  [51200/60000]\n",
      "loss: 0.407941  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.443820 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.221650  [    0/60000]\n",
      "loss: 0.371225  [ 6400/60000]\n",
      "loss: 0.294456  [12800/60000]\n",
      "loss: 0.416593  [19200/60000]\n",
      "loss: 0.478090  [25600/60000]\n",
      "loss: 0.296748  [32000/60000]\n",
      "loss: 0.312900  [38400/60000]\n",
      "loss: 0.472423  [44800/60000]\n",
      "loss: 0.464994  [51200/60000]\n",
      "loss: 0.404911  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.442039 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.219608  [    0/60000]\n",
      "loss: 0.369142  [ 6400/60000]\n",
      "loss: 0.293535  [12800/60000]\n",
      "loss: 0.413289  [19200/60000]\n",
      "loss: 0.472724  [25600/60000]\n",
      "loss: 0.292686  [32000/60000]\n",
      "loss: 0.311124  [38400/60000]\n",
      "loss: 0.473976  [44800/60000]\n",
      "loss: 0.463139  [51200/60000]\n",
      "loss: 0.402169  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.440374 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train the arcsin NN.\n",
    "\"\"\"\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, model, loss_fn, optimizer)\n",
    "    test_loop(test_loader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ApproxArcsineNN(\n",
       "  (Flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (Linears): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
       "    (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  )\n",
       "  (Output): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approx_model = ApproxArcsineNN(model).to(device)\n",
    "approx_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 36.0%, Avg loss: 3.267939 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loop(test_loader,approx_model,loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5*1e-3\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(approx_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.196406  [    0/60000]\n",
      "loss: 1.034924  [ 6400/60000]\n",
      "loss: 0.702310  [12800/60000]\n",
      "loss: 0.918660  [19200/60000]\n",
      "loss: 0.943056  [25600/60000]\n",
      "loss: 0.704924  [32000/60000]\n",
      "loss: 0.729974  [38400/60000]\n",
      "loss: 0.718146  [44800/60000]\n",
      "loss: 0.934986  [51200/60000]\n",
      "loss: 0.795974  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.855636 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.646713  [    0/60000]\n",
      "loss: 0.827655  [ 6400/60000]\n",
      "loss: 0.590898  [12800/60000]\n",
      "loss: 0.832639  [19200/60000]\n",
      "loss: 0.890666  [25600/60000]\n",
      "loss: 0.716206  [32000/60000]\n",
      "loss: 0.722073  [38400/60000]\n",
      "loss: 0.720469  [44800/60000]\n",
      "loss: 0.879605  [51200/60000]\n",
      "loss: 0.781933  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.3%, Avg loss: 0.826669 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.639981  [    0/60000]\n",
      "loss: 0.791992  [ 6400/60000]\n",
      "loss: 0.569536  [12800/60000]\n",
      "loss: 0.805711  [19200/60000]\n",
      "loss: 0.858490  [25600/60000]\n",
      "loss: 0.718397  [32000/60000]\n",
      "loss: 0.714085  [38400/60000]\n",
      "loss: 0.716817  [44800/60000]\n",
      "loss: 0.847587  [51200/60000]\n",
      "loss: 0.769320  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 70.7%, Avg loss: 0.812477 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.634959  [    0/60000]\n",
      "loss: 0.771554  [ 6400/60000]\n",
      "loss: 0.556736  [12800/60000]\n",
      "loss: 0.792559  [19200/60000]\n",
      "loss: 0.833283  [25600/60000]\n",
      "loss: 0.715762  [32000/60000]\n",
      "loss: 0.706537  [38400/60000]\n",
      "loss: 0.712265  [44800/60000]\n",
      "loss: 0.829264  [51200/60000]\n",
      "loss: 0.758642  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.803617 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.629977  [    0/60000]\n",
      "loss: 0.757234  [ 6400/60000]\n",
      "loss: 0.547876  [12800/60000]\n",
      "loss: 0.784176  [19200/60000]\n",
      "loss: 0.813242  [25600/60000]\n",
      "loss: 0.711910  [32000/60000]\n",
      "loss: 0.700065  [38400/60000]\n",
      "loss: 0.708143  [44800/60000]\n",
      "loss: 0.817582  [51200/60000]\n",
      "loss: 0.749957  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.797433 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.625215  [    0/60000]\n",
      "loss: 0.746297  [ 6400/60000]\n",
      "loss: 0.541385  [12800/60000]\n",
      "loss: 0.778312  [19200/60000]\n",
      "loss: 0.797101  [25600/60000]\n",
      "loss: 0.708101  [32000/60000]\n",
      "loss: 0.694679  [38400/60000]\n",
      "loss: 0.704613  [44800/60000]\n",
      "loss: 0.809440  [51200/60000]\n",
      "loss: 0.743008  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.792838 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.620848  [    0/60000]\n",
      "loss: 0.737529  [ 6400/60000]\n",
      "loss: 0.536428  [12800/60000]\n",
      "loss: 0.774090  [19200/60000]\n",
      "loss: 0.783928  [25600/60000]\n",
      "loss: 0.704717  [32000/60000]\n",
      "loss: 0.690254  [38400/60000]\n",
      "loss: 0.701594  [44800/60000]\n",
      "loss: 0.803391  [51200/60000]\n",
      "loss: 0.737468  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.789281 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.616932  [    0/60000]\n",
      "loss: 0.730276  [ 6400/60000]\n",
      "loss: 0.532498  [12800/60000]\n",
      "loss: 0.771021  [19200/60000]\n",
      "loss: 0.773024  [25600/60000]\n",
      "loss: 0.701835  [32000/60000]\n",
      "loss: 0.686641  [38400/60000]\n",
      "loss: 0.698975  [44800/60000]\n",
      "loss: 0.798691  [51200/60000]\n",
      "loss: 0.733039  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.786446 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.613455  [    0/60000]\n",
      "loss: 0.724143  [ 6400/60000]\n",
      "loss: 0.529272  [12800/60000]\n",
      "loss: 0.768784  [19200/60000]\n",
      "loss: 0.763866  [25600/60000]\n",
      "loss: 0.699429  [32000/60000]\n",
      "loss: 0.683700  [38400/60000]\n",
      "loss: 0.696670  [44800/60000]\n",
      "loss: 0.794923  [51200/60000]\n",
      "loss: 0.729477  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.784133 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.610377  [    0/60000]\n",
      "loss: 0.718871  [ 6400/60000]\n",
      "loss: 0.526544  [12800/60000]\n",
      "loss: 0.767156  [19200/60000]\n",
      "loss: 0.756064  [25600/60000]\n",
      "loss: 0.697440  [32000/60000]\n",
      "loss: 0.681312  [38400/60000]\n",
      "loss: 0.694617  [44800/60000]\n",
      "loss: 0.791830  [51200/60000]\n",
      "loss: 0.726588  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.782210 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.607647  [    0/60000]\n",
      "loss: 0.714280  [ 6400/60000]\n",
      "loss: 0.524179  [12800/60000]\n",
      "loss: 0.765978  [19200/60000]\n",
      "loss: 0.749329  [25600/60000]\n",
      "loss: 0.695808  [32000/60000]\n",
      "loss: 0.679377  [38400/60000]\n",
      "loss: 0.692769  [44800/60000]\n",
      "loss: 0.789246  [51200/60000]\n",
      "loss: 0.724224  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.780588 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.605217  [    0/60000]\n",
      "loss: 0.710242  [ 6400/60000]\n",
      "loss: 0.522085  [12800/60000]\n",
      "loss: 0.765136  [19200/60000]\n",
      "loss: 0.743443  [25600/60000]\n",
      "loss: 0.694477  [32000/60000]\n",
      "loss: 0.677811  [38400/60000]\n",
      "loss: 0.691094  [44800/60000]\n",
      "loss: 0.787058  [51200/60000]\n",
      "loss: 0.722273  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.6%, Avg loss: 0.779202 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.603044  [    0/60000]\n",
      "loss: 0.706657  [ 6400/60000]\n",
      "loss: 0.520203  [12800/60000]\n",
      "loss: 0.764546  [19200/60000]\n",
      "loss: 0.738240  [25600/60000]\n",
      "loss: 0.693399  [32000/60000]\n",
      "loss: 0.676548  [38400/60000]\n",
      "loss: 0.689567  [44800/60000]\n",
      "loss: 0.785184  [51200/60000]\n",
      "loss: 0.720648  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.778004 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.601088  [    0/60000]\n",
      "loss: 0.703451  [ 6400/60000]\n",
      "loss: 0.518488  [12800/60000]\n",
      "loss: 0.764147  [19200/60000]\n",
      "loss: 0.733597  [25600/60000]\n",
      "loss: 0.692533  [32000/60000]\n",
      "loss: 0.675533  [38400/60000]\n",
      "loss: 0.688170  [44800/60000]\n",
      "loss: 0.783566  [51200/60000]\n",
      "loss: 0.719282  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.776961 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.599318  [    0/60000]\n",
      "loss: 0.700566  [ 6400/60000]\n",
      "loss: 0.516910  [12800/60000]\n",
      "loss: 0.763895  [19200/60000]\n",
      "loss: 0.729416  [25600/60000]\n",
      "loss: 0.691843  [32000/60000]\n",
      "loss: 0.674720  [38400/60000]\n",
      "loss: 0.686886  [44800/60000]\n",
      "loss: 0.782158  [51200/60000]\n",
      "loss: 0.718124  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.776045 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.597708  [    0/60000]\n",
      "loss: 0.697953  [ 6400/60000]\n",
      "loss: 0.515446  [12800/60000]\n",
      "loss: 0.763755  [19200/60000]\n",
      "loss: 0.725622  [25600/60000]\n",
      "loss: 0.691300  [32000/60000]\n",
      "loss: 0.674075  [38400/60000]\n",
      "loss: 0.685702  [44800/60000]\n",
      "loss: 0.780926  [51200/60000]\n",
      "loss: 0.717134  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.775234 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.596236  [    0/60000]\n",
      "loss: 0.695574  [ 6400/60000]\n",
      "loss: 0.514079  [12800/60000]\n",
      "loss: 0.763701  [19200/60000]\n",
      "loss: 0.722157  [25600/60000]\n",
      "loss: 0.690881  [32000/60000]\n",
      "loss: 0.673568  [38400/60000]\n",
      "loss: 0.684607  [44800/60000]\n",
      "loss: 0.779842  [51200/60000]\n",
      "loss: 0.716280  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.7%, Avg loss: 0.774514 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.594883  [    0/60000]\n",
      "loss: 0.693398  [ 6400/60000]\n",
      "loss: 0.512797  [12800/60000]\n",
      "loss: 0.763714  [19200/60000]\n",
      "loss: 0.718973  [25600/60000]\n",
      "loss: 0.690564  [32000/60000]\n",
      "loss: 0.673175  [38400/60000]\n",
      "loss: 0.683592  [44800/60000]\n",
      "loss: 0.778884  [51200/60000]\n",
      "loss: 0.715537  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.773870 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.593635  [    0/60000]\n",
      "loss: 0.691397  [ 6400/60000]\n",
      "loss: 0.511589  [12800/60000]\n",
      "loss: 0.763779  [19200/60000]\n",
      "loss: 0.716032  [25600/60000]\n",
      "loss: 0.690331  [32000/60000]\n",
      "loss: 0.672877  [38400/60000]\n",
      "loss: 0.682648  [44800/60000]\n",
      "loss: 0.778034  [51200/60000]\n",
      "loss: 0.714885  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.773292 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.592478  [    0/60000]\n",
      "loss: 0.689551  [ 6400/60000]\n",
      "loss: 0.510447  [12800/60000]\n",
      "loss: 0.763883  [19200/60000]\n",
      "loss: 0.713303  [25600/60000]\n",
      "loss: 0.690169  [32000/60000]\n",
      "loss: 0.672657  [38400/60000]\n",
      "loss: 0.681768  [44800/60000]\n",
      "loss: 0.777278  [51200/60000]\n",
      "loss: 0.714307  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.8%, Avg loss: 0.772771 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, approx_model, loss_fn, optimizer)\n",
    "    test_loop(test_loader, approx_model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepresentArcsineNN(\n",
       "  (Flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (Linears): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
       "    (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  )\n",
       "  (Output): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composite_model = RepresentArcsineNN(model).to(device)\n",
    "composite_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 21.7%, Avg loss: 2.477477 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loop(test_loader,composite_model,loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5*1e-3\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(composite_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.464188  [    0/60000]\n",
      "loss: 1.463477  [ 6400/60000]\n",
      "loss: 0.953338  [12800/60000]\n",
      "loss: 1.205656  [19200/60000]\n",
      "loss: 0.873847  [25600/60000]\n",
      "loss: 1.017071  [32000/60000]\n",
      "loss: 0.904895  [38400/60000]\n",
      "loss: 0.802798  [44800/60000]\n",
      "loss: 0.957256  [51200/60000]\n",
      "loss: 0.843616  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.0%, Avg loss: 0.816899 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.656620  [    0/60000]\n",
      "loss: 1.014019  [ 6400/60000]\n",
      "loss: 0.550010  [12800/60000]\n",
      "loss: 0.983556  [19200/60000]\n",
      "loss: 0.716611  [25600/60000]\n",
      "loss: 0.833702  [32000/60000]\n",
      "loss: 0.779663  [38400/60000]\n",
      "loss: 0.751554  [44800/60000]\n",
      "loss: 0.873554  [51200/60000]\n",
      "loss: 0.751323  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.2%, Avg loss: 0.743759 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.575610  [    0/60000]\n",
      "loss: 0.926225  [ 6400/60000]\n",
      "loss: 0.481549  [12800/60000]\n",
      "loss: 0.923675  [19200/60000]\n",
      "loss: 0.660986  [25600/60000]\n",
      "loss: 0.773763  [32000/60000]\n",
      "loss: 0.732218  [38400/60000]\n",
      "loss: 0.738936  [44800/60000]\n",
      "loss: 0.839105  [51200/60000]\n",
      "loss: 0.711598  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.9%, Avg loss: 0.712904 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.539953  [    0/60000]\n",
      "loss: 0.879058  [ 6400/60000]\n",
      "loss: 0.451301  [12800/60000]\n",
      "loss: 0.888826  [19200/60000]\n",
      "loss: 0.627824  [25600/60000]\n",
      "loss: 0.741245  [32000/60000]\n",
      "loss: 0.706110  [38400/60000]\n",
      "loss: 0.732366  [44800/60000]\n",
      "loss: 0.818787  [51200/60000]\n",
      "loss: 0.688543  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.695338 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.517884  [    0/60000]\n",
      "loss: 0.847452  [ 6400/60000]\n",
      "loss: 0.433050  [12800/60000]\n",
      "loss: 0.864258  [19200/60000]\n",
      "loss: 0.604845  [25600/60000]\n",
      "loss: 0.719318  [32000/60000]\n",
      "loss: 0.689463  [38400/60000]\n",
      "loss: 0.727771  [44800/60000]\n",
      "loss: 0.805050  [51200/60000]\n",
      "loss: 0.673616  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.8%, Avg loss: 0.683864 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.502230  [    0/60000]\n",
      "loss: 0.823527  [ 6400/60000]\n",
      "loss: 0.420348  [12800/60000]\n",
      "loss: 0.845519  [19200/60000]\n",
      "loss: 0.587872  [25600/60000]\n",
      "loss: 0.702799  [32000/60000]\n",
      "loss: 0.678022  [38400/60000]\n",
      "loss: 0.724024  [44800/60000]\n",
      "loss: 0.794929  [51200/60000]\n",
      "loss: 0.663444  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.1%, Avg loss: 0.675747 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.490368  [    0/60000]\n",
      "loss: 0.804116  [ 6400/60000]\n",
      "loss: 0.410741  [12800/60000]\n",
      "loss: 0.830661  [19200/60000]\n",
      "loss: 0.574880  [25600/60000]\n",
      "loss: 0.689582  [32000/60000]\n",
      "loss: 0.669771  [38400/60000]\n",
      "loss: 0.720680  [44800/60000]\n",
      "loss: 0.787018  [51200/60000]\n",
      "loss: 0.656317  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.669696 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.481032  [    0/60000]\n",
      "loss: 0.787723  [ 6400/60000]\n",
      "loss: 0.403057  [12800/60000]\n",
      "loss: 0.818603  [19200/60000]\n",
      "loss: 0.564673  [25600/60000]\n",
      "loss: 0.678619  [32000/60000]\n",
      "loss: 0.663601  [38400/60000]\n",
      "loss: 0.717548  [44800/60000]\n",
      "loss: 0.780570  [51200/60000]\n",
      "loss: 0.651246  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.665013 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.473495  [    0/60000]\n",
      "loss: 0.773529  [ 6400/60000]\n",
      "loss: 0.396662  [12800/60000]\n",
      "loss: 0.808651  [19200/60000]\n",
      "loss: 0.556477  [25600/60000]\n",
      "loss: 0.669311  [32000/60000]\n",
      "loss: 0.658848  [38400/60000]\n",
      "loss: 0.714549  [44800/60000]\n",
      "loss: 0.775161  [51200/60000]\n",
      "loss: 0.647612  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.661285 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.467296  [    0/60000]\n",
      "loss: 0.761031  [ 6400/60000]\n",
      "loss: 0.391183  [12800/60000]\n",
      "loss: 0.800327  [19200/60000]\n",
      "loss: 0.549768  [25600/60000]\n",
      "loss: 0.661270  [32000/60000]\n",
      "loss: 0.655087  [38400/60000]\n",
      "loss: 0.711652  [44800/60000]\n",
      "loss: 0.770531  [51200/60000]\n",
      "loss: 0.645008  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.6%, Avg loss: 0.658249 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.462121  [    0/60000]\n",
      "loss: 0.749892  [ 6400/60000]\n",
      "loss: 0.386389  [12800/60000]\n",
      "loss: 0.793281  [19200/60000]\n",
      "loss: 0.544178  [25600/60000]\n",
      "loss: 0.654231  [32000/60000]\n",
      "loss: 0.652044  [38400/60000]\n",
      "loss: 0.708849  [44800/60000]\n",
      "loss: 0.766507  [51200/60000]\n",
      "loss: 0.643157  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.655732 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.457746  [    0/60000]\n",
      "loss: 0.739870  [ 6400/60000]\n",
      "loss: 0.382130  [12800/60000]\n",
      "loss: 0.787254  [19200/60000]\n",
      "loss: 0.539443  [25600/60000]\n",
      "loss: 0.648003  [32000/60000]\n",
      "loss: 0.649528  [38400/60000]\n",
      "loss: 0.706143  [44800/60000]\n",
      "loss: 0.762972  [51200/60000]\n",
      "loss: 0.641867  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.653613 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.454008  [    0/60000]\n",
      "loss: 0.730783  [ 6400/60000]\n",
      "loss: 0.378302  [12800/60000]\n",
      "loss: 0.782051  [19200/60000]\n",
      "loss: 0.535373  [25600/60000]\n",
      "loss: 0.642441  [32000/60000]\n",
      "loss: 0.647410  [38400/60000]\n",
      "loss: 0.703537  [44800/60000]\n",
      "loss: 0.759840  [51200/60000]\n",
      "loss: 0.640999  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.8%, Avg loss: 0.651807 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.450785  [    0/60000]\n",
      "loss: 0.722491  [ 6400/60000]\n",
      "loss: 0.374832  [12800/60000]\n",
      "loss: 0.777522  [19200/60000]\n",
      "loss: 0.531826  [25600/60000]\n",
      "loss: 0.637439  [32000/60000]\n",
      "loss: 0.645598  [38400/60000]\n",
      "loss: 0.701035  [44800/60000]\n",
      "loss: 0.757048  [51200/60000]\n",
      "loss: 0.640453  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.650250 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.447983  [    0/60000]\n",
      "loss: 0.714883  [ 6400/60000]\n",
      "loss: 0.371665  [12800/60000]\n",
      "loss: 0.773549  [19200/60000]\n",
      "loss: 0.528697  [25600/60000]\n",
      "loss: 0.632910  [32000/60000]\n",
      "loss: 0.644026  [38400/60000]\n",
      "loss: 0.698641  [44800/60000]\n",
      "loss: 0.754545  [51200/60000]\n",
      "loss: 0.640154  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.648896 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.445528  [    0/60000]\n",
      "loss: 0.707869  [ 6400/60000]\n",
      "loss: 0.368760  [12800/60000]\n",
      "loss: 0.770041  [19200/60000]\n",
      "loss: 0.525905  [25600/60000]\n",
      "loss: 0.628788  [32000/60000]\n",
      "loss: 0.642645  [38400/60000]\n",
      "loss: 0.696357  [44800/60000]\n",
      "loss: 0.752291  [51200/60000]\n",
      "loss: 0.640046  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.647706 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.443364  [    0/60000]\n",
      "loss: 0.701375  [ 6400/60000]\n",
      "loss: 0.366084  [12800/60000]\n",
      "loss: 0.766924  [19200/60000]\n",
      "loss: 0.523390  [25600/60000]\n",
      "loss: 0.625019  [32000/60000]\n",
      "loss: 0.641419  [38400/60000]\n",
      "loss: 0.694180  [44800/60000]\n",
      "loss: 0.750255  [51200/60000]\n",
      "loss: 0.640088  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.646655 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.441445  [    0/60000]\n",
      "loss: 0.695340  [ 6400/60000]\n",
      "loss: 0.363610  [12800/60000]\n",
      "loss: 0.764141  [19200/60000]\n",
      "loss: 0.521103  [25600/60000]\n",
      "loss: 0.621558  [32000/60000]\n",
      "loss: 0.640321  [38400/60000]\n",
      "loss: 0.692111  [44800/60000]\n",
      "loss: 0.748409  [51200/60000]\n",
      "loss: 0.640246  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.9%, Avg loss: 0.645718 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.439734  [    0/60000]\n",
      "loss: 0.689712  [ 6400/60000]\n",
      "loss: 0.361315  [12800/60000]\n",
      "loss: 0.761643  [19200/60000]\n",
      "loss: 0.519006  [25600/60000]\n",
      "loss: 0.618369  [32000/60000]\n",
      "loss: 0.639329  [38400/60000]\n",
      "loss: 0.690147  [44800/60000]\n",
      "loss: 0.746732  [51200/60000]\n",
      "loss: 0.640496  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.644879 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.438202  [    0/60000]\n",
      "loss: 0.684448  [ 6400/60000]\n",
      "loss: 0.359182  [12800/60000]\n",
      "loss: 0.759391  [19200/60000]\n",
      "loss: 0.517071  [25600/60000]\n",
      "loss: 0.615422  [32000/60000]\n",
      "loss: 0.638428  [38400/60000]\n",
      "loss: 0.688284  [44800/60000]\n",
      "loss: 0.745203  [51200/60000]\n",
      "loss: 0.640816  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.644124 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, composite_model, loss_fn, optimizer)\n",
    "    test_loop(test_loader, composite_model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('DL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07315ba070dea1980208edb5f4fd988df0f55a182694c2201b33f2c556751b3c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
