{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import asin\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Download data from FashionMNIST\n",
    "\"\"\"\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Arcsine(nn.Linear):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True, device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(Linear_Arcsine, self).__init__(in_features, out_features, bias, device, dtype)\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.empty((out_features,1), **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "    def reset_parameters(self) -> None:\n",
    "        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n",
    "        # uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\n",
    "        # https://github.com/pytorch/pytorch/issues/57109\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        if self.bias is not None:\n",
    "            W = torch.concat([self.weight, self.bias], dim = 1)\n",
    "            input = torch.concat([input, torch.ones((input.shape[0],1),device=device)], dim=1)\n",
    "        else:\n",
    "            W = self.weight\n",
    "        return torch.asin(nn.functional.normalize(input, dim = 1) @ nn.functional.normalize(W, dim = 1).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "arcsin\n",
    "nn with arcsin activation\n",
    "\"\"\"\n",
    "class arcsinNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(arcsinNN, self).__init__()\n",
    "        self.Flatten = nn.Flatten() # flatten the input\n",
    "        self.Layers = nn.Sequential(\n",
    "            Linear_Arcsine(28*28, 1024),\n",
    "            Linear_Arcsine(1024, 1024),\n",
    "            Linear_Arcsine(1024,512),\n",
    "            nn.Linear(512,10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "       \n",
    "        x = self.Flatten(x)\n",
    "        logits = self.Layers(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "arcsin\n",
    "nn with arcsin activation\n",
    "Layer 1: d_in:784, d_out:512 with arcsin activation\n",
    "Layer 2: d_in:512, d_out:10 with no activation\n",
    "\"\"\"\n",
    "from torch import arcsin\n",
    "\n",
    "\n",
    "class arcsinNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(arcsinNN, self).__init__()\n",
    "        self.Flatten = nn.Flatten() # flatten the input\n",
    "        self.Linear1 = nn.Linear(28*28,512, bias = False)\n",
    "        self.Linear2 = nn.Linear(512,10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "       \n",
    "        x_flat = self.Flatten(x)\n",
    "\n",
    "        x1 = self.Linear1(x_flat)\n",
    "\n",
    "        x2 = torch.tensor(x_flat.T) # [d, n]\n",
    "\n",
    "        W = self.Linear1.weight.T # [d, D]\n",
    "\n",
    "        \n",
    "        x = torch.arcsin(nn.functional.normalize(W, dim = 0).T @ nn.functional.normalize(x2, dim = 0) )\n",
    "        \n",
    "        # second layer\n",
    "        logits = self.Linear2(x.T)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arcsinNN(\n",
      "  (Flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (Layers): Sequential(\n",
      "    (0): Linear_Arcsine(in_features=784, out_features=1024, bias=True)\n",
      "    (1): Linear_Arcsine(in_features=1024, out_features=1024, bias=True)\n",
      "    (2): Linear_Arcsine(in_features=1024, out_features=512, bias=True)\n",
      "    (3): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "arcsin_model = arcsinNN().to(device)\n",
    "print(arcsin_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5*1e-3\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=arcsin_model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(training_data,batch_size=batch_size,collate_fn=lambda x:tuple(x_.to(device) for x_ in default_collate(x)))\n",
    "test_loader = DataLoader(test_data,batch_size=batch_size,collate_fn=lambda x:tuple(x_.to(device) for x_ in default_collate(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X.float())\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X.float())\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.788042  [    0/60000]\n",
      "loss: 0.792586  [ 6400/60000]\n",
      "loss: 0.594242  [12800/60000]\n",
      "loss: 0.744619  [19200/60000]\n",
      "loss: 0.606815  [25600/60000]\n",
      "loss: 0.507904  [32000/60000]\n",
      "loss: 0.541784  [38400/60000]\n",
      "loss: 0.656298  [44800/60000]\n",
      "loss: 0.511460  [51200/60000]\n",
      "loss: 0.621085  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.9%, Avg loss: 0.560171 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.455348  [    0/60000]\n",
      "loss: 0.522918  [ 6400/60000]\n",
      "loss: 0.435473  [12800/60000]\n",
      "loss: 0.526622  [19200/60000]\n",
      "loss: 0.601740  [25600/60000]\n",
      "loss: 0.446505  [32000/60000]\n",
      "loss: 0.421663  [38400/60000]\n",
      "loss: 0.581674  [44800/60000]\n",
      "loss: 0.526054  [51200/60000]\n",
      "loss: 0.506725  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.8%, Avg loss: 0.552932 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.430264  [    0/60000]\n",
      "loss: 0.486433  [ 6400/60000]\n",
      "loss: 0.350681  [12800/60000]\n",
      "loss: 0.464652  [19200/60000]\n",
      "loss: 0.564004  [25600/60000]\n",
      "loss: 0.403191  [32000/60000]\n",
      "loss: 0.367577  [38400/60000]\n",
      "loss: 0.545761  [44800/60000]\n",
      "loss: 0.501955  [51200/60000]\n",
      "loss: 0.499936  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 80.6%, Avg loss: 0.539051 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.397954  [    0/60000]\n",
      "loss: 0.477501  [ 6400/60000]\n",
      "loss: 0.323034  [12800/60000]\n",
      "loss: 0.455777  [19200/60000]\n",
      "loss: 0.559980  [25600/60000]\n",
      "loss: 0.396608  [32000/60000]\n",
      "loss: 0.350143  [38400/60000]\n",
      "loss: 0.535545  [44800/60000]\n",
      "loss: 0.489942  [51200/60000]\n",
      "loss: 0.479678  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.512022 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.363867  [    0/60000]\n",
      "loss: 0.457355  [ 6400/60000]\n",
      "loss: 0.320849  [12800/60000]\n",
      "loss: 0.444531  [19200/60000]\n",
      "loss: 0.554035  [25600/60000]\n",
      "loss: 0.390756  [32000/60000]\n",
      "loss: 0.343410  [38400/60000]\n",
      "loss: 0.528684  [44800/60000]\n",
      "loss: 0.481235  [51200/60000]\n",
      "loss: 0.460930  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.0%, Avg loss: 0.492491 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.336156  [    0/60000]\n",
      "loss: 0.449727  [ 6400/60000]\n",
      "loss: 0.321004  [12800/60000]\n",
      "loss: 0.440771  [19200/60000]\n",
      "loss: 0.544569  [25600/60000]\n",
      "loss: 0.375081  [32000/60000]\n",
      "loss: 0.332596  [38400/60000]\n",
      "loss: 0.522541  [44800/60000]\n",
      "loss: 0.476405  [51200/60000]\n",
      "loss: 0.445837  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.3%, Avg loss: 0.480616 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.318266  [    0/60000]\n",
      "loss: 0.444213  [ 6400/60000]\n",
      "loss: 0.321564  [12800/60000]\n",
      "loss: 0.440038  [19200/60000]\n",
      "loss: 0.533672  [25600/60000]\n",
      "loss: 0.355347  [32000/60000]\n",
      "loss: 0.325275  [38400/60000]\n",
      "loss: 0.516582  [44800/60000]\n",
      "loss: 0.474835  [51200/60000]\n",
      "loss: 0.434158  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.6%, Avg loss: 0.473005 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.307322  [    0/60000]\n",
      "loss: 0.437116  [ 6400/60000]\n",
      "loss: 0.322457  [12800/60000]\n",
      "loss: 0.439538  [19200/60000]\n",
      "loss: 0.516033  [25600/60000]\n",
      "loss: 0.336457  [32000/60000]\n",
      "loss: 0.319658  [38400/60000]\n",
      "loss: 0.507575  [44800/60000]\n",
      "loss: 0.475894  [51200/60000]\n",
      "loss: 0.422778  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.468913 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.299645  [    0/60000]\n",
      "loss: 0.437326  [ 6400/60000]\n",
      "loss: 0.321216  [12800/60000]\n",
      "loss: 0.440981  [19200/60000]\n",
      "loss: 0.473799  [25600/60000]\n",
      "loss: 0.322958  [32000/60000]\n",
      "loss: 0.323537  [38400/60000]\n",
      "loss: 0.499285  [44800/60000]\n",
      "loss: 0.472024  [51200/60000]\n",
      "loss: 0.417890  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.8%, Avg loss: 0.468220 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.280452  [    0/60000]\n",
      "loss: 0.432258  [ 6400/60000]\n",
      "loss: 0.323121  [12800/60000]\n",
      "loss: 0.438052  [19200/60000]\n",
      "loss: 0.463449  [25600/60000]\n",
      "loss: 0.317712  [32000/60000]\n",
      "loss: 0.324362  [38400/60000]\n",
      "loss: 0.497689  [44800/60000]\n",
      "loss: 0.469272  [51200/60000]\n",
      "loss: 0.415537  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.9%, Avg loss: 0.467151 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.271768  [    0/60000]\n",
      "loss: 0.432315  [ 6400/60000]\n",
      "loss: 0.324090  [12800/60000]\n",
      "loss: 0.433793  [19200/60000]\n",
      "loss: 0.468640  [25600/60000]\n",
      "loss: 0.316709  [32000/60000]\n",
      "loss: 0.326990  [38400/60000]\n",
      "loss: 0.494129  [44800/60000]\n",
      "loss: 0.463323  [51200/60000]\n",
      "loss: 0.413675  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.463736 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.267072  [    0/60000]\n",
      "loss: 0.437816  [ 6400/60000]\n",
      "loss: 0.321941  [12800/60000]\n",
      "loss: 0.432121  [19200/60000]\n",
      "loss: 0.474982  [25600/60000]\n",
      "loss: 0.318955  [32000/60000]\n",
      "loss: 0.334975  [38400/60000]\n",
      "loss: 0.490409  [44800/60000]\n",
      "loss: 0.458703  [51200/60000]\n",
      "loss: 0.403789  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.7%, Avg loss: 0.455062 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.259823  [    0/60000]\n",
      "loss: 0.433200  [ 6400/60000]\n",
      "loss: 0.320062  [12800/60000]\n",
      "loss: 0.430009  [19200/60000]\n",
      "loss: 0.473252  [25600/60000]\n",
      "loss: 0.320203  [32000/60000]\n",
      "loss: 0.338584  [38400/60000]\n",
      "loss: 0.489379  [44800/60000]\n",
      "loss: 0.458114  [51200/60000]\n",
      "loss: 0.397069  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.449004 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.253792  [    0/60000]\n",
      "loss: 0.420070  [ 6400/60000]\n",
      "loss: 0.320176  [12800/60000]\n",
      "loss: 0.427177  [19200/60000]\n",
      "loss: 0.470495  [25600/60000]\n",
      "loss: 0.315223  [32000/60000]\n",
      "loss: 0.338477  [38400/60000]\n",
      "loss: 0.489259  [44800/60000]\n",
      "loss: 0.458885  [51200/60000]\n",
      "loss: 0.393049  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.1%, Avg loss: 0.445114 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.249084  [    0/60000]\n",
      "loss: 0.409172  [ 6400/60000]\n",
      "loss: 0.320160  [12800/60000]\n",
      "loss: 0.424638  [19200/60000]\n",
      "loss: 0.465384  [25600/60000]\n",
      "loss: 0.309828  [32000/60000]\n",
      "loss: 0.336528  [38400/60000]\n",
      "loss: 0.488714  [44800/60000]\n",
      "loss: 0.459180  [51200/60000]\n",
      "loss: 0.390455  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.2%, Avg loss: 0.442017 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.245440  [    0/60000]\n",
      "loss: 0.401891  [ 6400/60000]\n",
      "loss: 0.320083  [12800/60000]\n",
      "loss: 0.422507  [19200/60000]\n",
      "loss: 0.461081  [25600/60000]\n",
      "loss: 0.305075  [32000/60000]\n",
      "loss: 0.334272  [38400/60000]\n",
      "loss: 0.487794  [44800/60000]\n",
      "loss: 0.458601  [51200/60000]\n",
      "loss: 0.388250  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.3%, Avg loss: 0.439020 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.242009  [    0/60000]\n",
      "loss: 0.396440  [ 6400/60000]\n",
      "loss: 0.319526  [12800/60000]\n",
      "loss: 0.420407  [19200/60000]\n",
      "loss: 0.456680  [25600/60000]\n",
      "loss: 0.301001  [32000/60000]\n",
      "loss: 0.332127  [38400/60000]\n",
      "loss: 0.485818  [44800/60000]\n",
      "loss: 0.457348  [51200/60000]\n",
      "loss: 0.385570  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.4%, Avg loss: 0.436161 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.238751  [    0/60000]\n",
      "loss: 0.391657  [ 6400/60000]\n",
      "loss: 0.318781  [12800/60000]\n",
      "loss: 0.418575  [19200/60000]\n",
      "loss: 0.452492  [25600/60000]\n",
      "loss: 0.296860  [32000/60000]\n",
      "loss: 0.330421  [38400/60000]\n",
      "loss: 0.482967  [44800/60000]\n",
      "loss: 0.456004  [51200/60000]\n",
      "loss: 0.382981  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.433738 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.235632  [    0/60000]\n",
      "loss: 0.386739  [ 6400/60000]\n",
      "loss: 0.317442  [12800/60000]\n",
      "loss: 0.416929  [19200/60000]\n",
      "loss: 0.448463  [25600/60000]\n",
      "loss: 0.292704  [32000/60000]\n",
      "loss: 0.329295  [38400/60000]\n",
      "loss: 0.480367  [44800/60000]\n",
      "loss: 0.454632  [51200/60000]\n",
      "loss: 0.379942  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.431403 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.233126  [    0/60000]\n",
      "loss: 0.382129  [ 6400/60000]\n",
      "loss: 0.316111  [12800/60000]\n",
      "loss: 0.416098  [19200/60000]\n",
      "loss: 0.444220  [25600/60000]\n",
      "loss: 0.288903  [32000/60000]\n",
      "loss: 0.328491  [38400/60000]\n",
      "loss: 0.478467  [44800/60000]\n",
      "loss: 0.453022  [51200/60000]\n",
      "loss: 0.376328  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.6%, Avg loss: 0.429629 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train the arcsin NN.\n",
    "\"\"\"\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, arcsin_model, loss_fn, optimizer)\n",
    "    test_loop(test_loader, arcsin_model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator = lambda x : (x > 0).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "random feature map for arcsin\n",
    "\"\"\"\n",
    "class RandomFeatureMap(nn.Module):\n",
    "    def __init__(self, size_in):\n",
    "        \"\"\"\n",
    "        Build a feature map with output dimension equals input dimension\n",
    "\n",
    "        size_in: input dimension d\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.size_in = size_in\n",
    "        \"\"\"\n",
    "        weights is d by d Gaussian matrix \n",
    "        \"\"\"\n",
    "        weights = [nn.Parameter(torch.tensor(np.random.normal(loc=0.0, scale=1.0, size=(self.size_in,self.size_in)),device=device), requires_grad=False)] # for each N, generate N random Rademacher vectors\n",
    "        self.weights = nn.ParameterList(weights)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        return feature map for arcsin: for each Z in weights matrix have the feature sign(Z dot x) = 2*indicator(Z dot x) -1\n",
    "        with these features, phix^T phiy estimates d*(2/pi)arcsin(x^T y / ||x||||y||) \n",
    "        \"\"\"\n",
    "\n",
    "        return torch.stack([2*indicator(x @ weight)-1 for weight in self.weights]).squeeze(0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 784])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sanity check\n",
    "\"\"\"\n",
    "F = RandomFeatureMap(784).float()\n",
    "x = torch.rand((20,784))\n",
    "print(F(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = arcsin_model.get_parameter(target='Linear1.weight').shape[0]\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 784]) 0\n",
      "torch.Size([1024, 1]) 1\n",
      "torch.Size([1024, 1024]) 2\n",
      "torch.Size([1024, 1]) 3\n",
      "torch.Size([512, 1024]) 4\n",
      "torch.Size([512, 1]) 5\n",
      "torch.Size([10, 512]) 6\n",
      "torch.Size([10]) 7\n"
     ]
    }
   ],
   "source": [
    "for i,p in enumerate(arcsin_model.parameters()):\n",
    "    print(p.shape,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "approximate the arcsin nn using the feature map\n",
    "\"\"\"\n",
    "class ApproxNN(nn.Module):\n",
    "    def __init__(self, model: nn.Module = None):\n",
    "        super(ApproxNN, self).__init__()\n",
    "        self.Flatten = nn.Flatten() # flatten the input\n",
    "        \n",
    "        # # Initialize two linear layers with weights from the trained arcsin neural network.\n",
    "        # self.Linear1 = nn.Linear(28*28,1024)\n",
    "        # self.Linear1.weight = nn.Parameter(torch.clone(arcsin_model.get_parameter(target='Linear1.weight')))\n",
    "        # self.Linear2 = nn.Linear(512,10)\n",
    "        # self.Linear2.weight = nn.Parameter(torch.clone(arcsin_model.get_parameter(target='Linear2.weight')))\n",
    "        # self.Linear2.bias = nn.Parameter(torch.clone(arcsin_model.get_parameter(target='Linear2.bias')))\n",
    "        self.Linear_Arcsines = nn.ModuleList([nn.Linear(28*28,1024), nn.Linear(1024,1024), nn.Linear(1024, 512)])\n",
    "        self.Linear = nn.Linear(512,10)\n",
    "        self.RandomFeatureMaps = [RandomFeatureMap(28*28 + 1).float(), RandomFeatureMap(1024 + 1).float()] # Initialize the random feature map\n",
    "        if model is not None:\n",
    "            params = [x for x in model.parameters()]\n",
    "            self.Linear_Arcsines[0].weight, self.Linear_Arcsines[0].bias = params[0], params[1]\n",
    "            self.Linear_Arcsines[1].weight, self.Linear_Arcsines[1].bias = params[2], params[3]\n",
    "            self.Linear_Arcsines[2].weight, self.Linear_Arcsines[2].bias = params[4], params[5]\n",
    "            self.Linear.weight, self.Linear.bias = params[6], params[7]\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.Flatten(x) # [n, d]\n",
    "        D = x.shape[1]\n",
    "        x = torch.concat([x, torch.ones((x.shape[0],1),device=device)], dim=1) # [n, d+1]\n",
    "        phi_x1 = self.RandomFeatureMaps[0](x)  # [n, d+1]\n",
    "        W1 = torch.concat([self.Linear_Arcsines[0].weight, self.Linear_Arcsines[0].bias.reshape(-1,1)], dim = 1) # [D, d+1]\n",
    "        phi_W1 = self.RandomFeatureMaps[0](W1)  # [D, d+1]\n",
    "        x = (np.pi/2)*((phi_x1 @ phi_W1.T)/D) # [n, D]\n",
    "\n",
    "        x = torch.concat([x, torch.ones((x.shape[0],1),device=device)], dim=1) # [n, D+1]\n",
    "        D = x.shape[1]\n",
    "        phi_x2 = self.RandomFeatureMaps[1](x) # [n, D+1]\n",
    "        W2 = torch.concat([self.Linear_Arcsines[1].weight, self.Linear_Arcsines[1].bias.reshape(-1,1)], dim = 1) # [D, D+1]\n",
    "        phi_W2 = self.RandomFeatureMaps[1](W2)\n",
    "        x = (np.pi/2)*((phi_x2 @ phi_W2.T)/D) # [n, D]\n",
    "\n",
    "        x = torch.concat([x, torch.ones((x.shape[0],1),device=device)], dim=1) # [n, D+1]\n",
    "        D = x.shape[1]\n",
    "        phi_x3= self.RandomFeatureMaps[1](x) # [n, D+1]\n",
    "        W3 = torch.concat([self.Linear_Arcsines[2].weight, self.Linear_Arcsines[2].bias.reshape(-1,1)], dim = 1) # [D, D+1]\n",
    "        phi_W3 = self.RandomFeatureMaps[1](W3)\n",
    "        x = (np.pi/2)*((phi_x3 @ phi_W3.T)/D) # [n, D]\n",
    "\n",
    "\n",
    "        # output layer\n",
    "        logits = self.Linear(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ApproxNN(\n",
       "  (Flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (Linear_Arcsines): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
       "    (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  )\n",
       "  (Linear): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approx_model = ApproxNN(arcsin_model).to(device)\n",
    "approx_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 36.8%, Avg loss: 3.193804 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loop(test_loader,approx_model,loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5*1e-3\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(approx_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.805723  [    0/60000]\n",
      "loss: 0.902240  [ 6400/60000]\n",
      "loss: 0.787267  [12800/60000]\n",
      "loss: 1.077304  [19200/60000]\n",
      "loss: 0.799184  [25600/60000]\n",
      "loss: 0.807200  [32000/60000]\n",
      "loss: 0.851136  [38400/60000]\n",
      "loss: 0.780177  [44800/60000]\n",
      "loss: 0.707347  [51200/60000]\n",
      "loss: 0.846230  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.0%, Avg loss: 0.795850 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.618774  [    0/60000]\n",
      "loss: 0.739327  [ 6400/60000]\n",
      "loss: 0.665057  [12800/60000]\n",
      "loss: 0.999990  [19200/60000]\n",
      "loss: 0.723274  [25600/60000]\n",
      "loss: 0.777493  [32000/60000]\n",
      "loss: 0.825820  [38400/60000]\n",
      "loss: 0.762647  [44800/60000]\n",
      "loss: 0.677188  [51200/60000]\n",
      "loss: 0.795998  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 0.770942 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.603661  [    0/60000]\n",
      "loss: 0.709153  [ 6400/60000]\n",
      "loss: 0.632986  [12800/60000]\n",
      "loss: 0.970057  [19200/60000]\n",
      "loss: 0.689899  [25600/60000]\n",
      "loss: 0.764959  [32000/60000]\n",
      "loss: 0.810259  [38400/60000]\n",
      "loss: 0.753671  [44800/60000]\n",
      "loss: 0.660738  [51200/60000]\n",
      "loss: 0.769361  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.1%, Avg loss: 0.758927 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.594071  [    0/60000]\n",
      "loss: 0.692740  [ 6400/60000]\n",
      "loss: 0.616397  [12800/60000]\n",
      "loss: 0.953260  [19200/60000]\n",
      "loss: 0.673317  [25600/60000]\n",
      "loss: 0.756768  [32000/60000]\n",
      "loss: 0.799467  [38400/60000]\n",
      "loss: 0.746878  [44800/60000]\n",
      "loss: 0.650346  [51200/60000]\n",
      "loss: 0.753583  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.751504 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.586630  [    0/60000]\n",
      "loss: 0.682016  [ 6400/60000]\n",
      "loss: 0.605444  [12800/60000]\n",
      "loss: 0.942141  [19200/60000]\n",
      "loss: 0.663302  [25600/60000]\n",
      "loss: 0.750759  [32000/60000]\n",
      "loss: 0.791777  [38400/60000]\n",
      "loss: 0.741339  [44800/60000]\n",
      "loss: 0.643065  [51200/60000]\n",
      "loss: 0.743446  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.746421 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.580626  [    0/60000]\n",
      "loss: 0.674270  [ 6400/60000]\n",
      "loss: 0.597202  [12800/60000]\n",
      "loss: 0.934142  [19200/60000]\n",
      "loss: 0.656326  [25600/60000]\n",
      "loss: 0.746143  [32000/60000]\n",
      "loss: 0.786055  [38400/60000]\n",
      "loss: 0.736680  [44800/60000]\n",
      "loss: 0.637515  [51200/60000]\n",
      "loss: 0.736546  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.742737 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.575748  [    0/60000]\n",
      "loss: 0.668346  [ 6400/60000]\n",
      "loss: 0.590548  [12800/60000]\n",
      "loss: 0.928076  [19200/60000]\n",
      "loss: 0.650999  [25600/60000]\n",
      "loss: 0.742457  [32000/60000]\n",
      "loss: 0.781623  [38400/60000]\n",
      "loss: 0.732673  [44800/60000]\n",
      "loss: 0.633015  [51200/60000]\n",
      "loss: 0.731637  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.7%, Avg loss: 0.739966 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.571763  [    0/60000]\n",
      "loss: 0.663665  [ 6400/60000]\n",
      "loss: 0.584960  [12800/60000]\n",
      "loss: 0.923279  [19200/60000]\n",
      "loss: 0.646687  [25600/60000]\n",
      "loss: 0.739413  [32000/60000]\n",
      "loss: 0.778077  [38400/60000]\n",
      "loss: 0.729165  [44800/60000]\n",
      "loss: 0.629201  [51200/60000]\n",
      "loss: 0.728001  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.8%, Avg loss: 0.737828 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.568480  [    0/60000]\n",
      "loss: 0.659888  [ 6400/60000]\n",
      "loss: 0.580152  [12800/60000]\n",
      "loss: 0.919347  [19200/60000]\n",
      "loss: 0.643060  [25600/60000]\n",
      "loss: 0.736832  [32000/60000]\n",
      "loss: 0.775169  [38400/60000]\n",
      "loss: 0.726053  [44800/60000]\n",
      "loss: 0.625861  [51200/60000]\n",
      "loss: 0.725203  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.736142 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.565749  [    0/60000]\n",
      "loss: 0.656798  [ 6400/60000]\n",
      "loss: 0.575947  [12800/60000]\n",
      "loss: 0.916020  [19200/60000]\n",
      "loss: 0.639927  [25600/60000]\n",
      "loss: 0.734601  [32000/60000]\n",
      "loss: 0.772739  [38400/60000]\n",
      "loss: 0.723264  [44800/60000]\n",
      "loss: 0.622868  [51200/60000]\n",
      "loss: 0.722965  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.734792 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.563453  [    0/60000]\n",
      "loss: 0.654240  [ 6400/60000]\n",
      "loss: 0.572226  [12800/60000]\n",
      "loss: 0.913134  [19200/60000]\n",
      "loss: 0.637165  [25600/60000]\n",
      "loss: 0.732644  [32000/60000]\n",
      "loss: 0.770677  [38400/60000]\n",
      "loss: 0.720744  [44800/60000]\n",
      "loss: 0.620137  [51200/60000]\n",
      "loss: 0.721105  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.733695 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.561502  [    0/60000]\n",
      "loss: 0.652102  [ 6400/60000]\n",
      "loss: 0.568902  [12800/60000]\n",
      "loss: 0.910573  [19200/60000]\n",
      "loss: 0.634695  [25600/60000]\n",
      "loss: 0.730911  [32000/60000]\n",
      "loss: 0.768908  [38400/60000]\n",
      "loss: 0.718451  [44800/60000]\n",
      "loss: 0.617615  [51200/60000]\n",
      "loss: 0.719503  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.732794 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.559827  [    0/60000]\n",
      "loss: 0.650300  [ 6400/60000]\n",
      "loss: 0.565912  [12800/60000]\n",
      "loss: 0.908260  [19200/60000]\n",
      "loss: 0.632459  [25600/60000]\n",
      "loss: 0.729365  [32000/60000]\n",
      "loss: 0.767376  [38400/60000]\n",
      "loss: 0.716353  [44800/60000]\n",
      "loss: 0.615264  [51200/60000]\n",
      "loss: 0.718077  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.0%, Avg loss: 0.732045 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.558374  [    0/60000]\n",
      "loss: 0.648769  [ 6400/60000]\n",
      "loss: 0.563206  [12800/60000]\n",
      "loss: 0.906142  [19200/60000]\n",
      "loss: 0.630415  [25600/60000]\n",
      "loss: 0.727978  [32000/60000]\n",
      "loss: 0.766040  [38400/60000]\n",
      "loss: 0.714425  [44800/60000]\n",
      "loss: 0.613057  [51200/60000]\n",
      "loss: 0.716775  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.731418 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.557100  [    0/60000]\n",
      "loss: 0.647462  [ 6400/60000]\n",
      "loss: 0.560744  [12800/60000]\n",
      "loss: 0.904177  [19200/60000]\n",
      "loss: 0.628533  [25600/60000]\n",
      "loss: 0.726728  [32000/60000]\n",
      "loss: 0.764868  [38400/60000]\n",
      "loss: 0.712644  [44800/60000]\n",
      "loss: 0.610974  [51200/60000]\n",
      "loss: 0.715558  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.730887 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.555973  [    0/60000]\n",
      "loss: 0.646338  [ 6400/60000]\n",
      "loss: 0.558495  [12800/60000]\n",
      "loss: 0.902337  [19200/60000]\n",
      "loss: 0.626789  [25600/60000]\n",
      "loss: 0.725596  [32000/60000]\n",
      "loss: 0.763834  [38400/60000]\n",
      "loss: 0.710993  [44800/60000]\n",
      "loss: 0.609002  [51200/60000]\n",
      "loss: 0.714403  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.730436 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.554967  [    0/60000]\n",
      "loss: 0.645368  [ 6400/60000]\n",
      "loss: 0.556432  [12800/60000]\n",
      "loss: 0.900601  [19200/60000]\n",
      "loss: 0.625165  [25600/60000]\n",
      "loss: 0.724567  [32000/60000]\n",
      "loss: 0.762919  [38400/60000]\n",
      "loss: 0.709457  [44800/60000]\n",
      "loss: 0.607129  [51200/60000]\n",
      "loss: 0.713291  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.1%, Avg loss: 0.730050 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.554060  [    0/60000]\n",
      "loss: 0.644526  [ 6400/60000]\n",
      "loss: 0.554534  [12800/60000]\n",
      "loss: 0.898951  [19200/60000]\n",
      "loss: 0.623646  [25600/60000]\n",
      "loss: 0.723629  [32000/60000]\n",
      "loss: 0.762107  [38400/60000]\n",
      "loss: 0.708022  [44800/60000]\n",
      "loss: 0.605346  [51200/60000]\n",
      "loss: 0.712212  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.729717 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.553237  [    0/60000]\n",
      "loss: 0.643794  [ 6400/60000]\n",
      "loss: 0.552782  [12800/60000]\n",
      "loss: 0.897375  [19200/60000]\n",
      "loss: 0.622220  [25600/60000]\n",
      "loss: 0.722771  [32000/60000]\n",
      "loss: 0.761383  [38400/60000]\n",
      "loss: 0.706679  [44800/60000]\n",
      "loss: 0.603646  [51200/60000]\n",
      "loss: 0.711158  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.729429 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.552484  [    0/60000]\n",
      "loss: 0.643155  [ 6400/60000]\n",
      "loss: 0.551161  [12800/60000]\n",
      "loss: 0.895862  [19200/60000]\n",
      "loss: 0.620877  [25600/60000]\n",
      "loss: 0.721981  [32000/60000]\n",
      "loss: 0.760737  [38400/60000]\n",
      "loss: 0.705416  [44800/60000]\n",
      "loss: 0.602023  [51200/60000]\n",
      "loss: 0.710125  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.2%, Avg loss: 0.729178 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, approx_model, loss_fn, optimizer)\n",
    "    test_loop(test_loader, approx_model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ApproxNN(\n",
       "  (Flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (Linear_Arcsines): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=1024, bias=True)\n",
       "    (1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  )\n",
       "  (Linear): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approx_model_2 = ApproxNN().to(device)\n",
    "approx_model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5*1e-3\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(approx_model_2.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.309504  [    0/60000]\n",
      "loss: 1.278397  [ 6400/60000]\n",
      "loss: 0.853149  [12800/60000]\n",
      "loss: 0.977041  [19200/60000]\n",
      "loss: 0.830342  [25600/60000]\n",
      "loss: 0.846256  [32000/60000]\n",
      "loss: 0.857329  [38400/60000]\n",
      "loss: 0.803575  [44800/60000]\n",
      "loss: 0.769239  [51200/60000]\n",
      "loss: 0.762467  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 73.6%, Avg loss: 0.735464 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.638553  [    0/60000]\n",
      "loss: 0.810593  [ 6400/60000]\n",
      "loss: 0.507958  [12800/60000]\n",
      "loss: 0.758793  [19200/60000]\n",
      "loss: 0.715028  [25600/60000]\n",
      "loss: 0.726271  [32000/60000]\n",
      "loss: 0.771497  [38400/60000]\n",
      "loss: 0.778427  [44800/60000]\n",
      "loss: 0.736700  [51200/60000]\n",
      "loss: 0.701074  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.690089 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.577960  [    0/60000]\n",
      "loss: 0.756343  [ 6400/60000]\n",
      "loss: 0.461069  [12800/60000]\n",
      "loss: 0.705386  [19200/60000]\n",
      "loss: 0.672133  [25600/60000]\n",
      "loss: 0.674270  [32000/60000]\n",
      "loss: 0.732666  [38400/60000]\n",
      "loss: 0.772141  [44800/60000]\n",
      "loss: 0.726261  [51200/60000]\n",
      "loss: 0.673339  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.2%, Avg loss: 0.671369 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.550223  [    0/60000]\n",
      "loss: 0.725836  [ 6400/60000]\n",
      "loss: 0.442088  [12800/60000]\n",
      "loss: 0.678284  [19200/60000]\n",
      "loss: 0.647609  [25600/60000]\n",
      "loss: 0.641820  [32000/60000]\n",
      "loss: 0.709982  [38400/60000]\n",
      "loss: 0.767380  [44800/60000]\n",
      "loss: 0.719183  [51200/60000]\n",
      "loss: 0.658568  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.5%, Avg loss: 0.660879 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.532783  [    0/60000]\n",
      "loss: 0.706987  [ 6400/60000]\n",
      "loss: 0.432060  [12800/60000]\n",
      "loss: 0.660496  [19200/60000]\n",
      "loss: 0.632351  [25600/60000]\n",
      "loss: 0.619270  [32000/60000]\n",
      "loss: 0.695013  [38400/60000]\n",
      "loss: 0.763222  [44800/60000]\n",
      "loss: 0.713817  [51200/60000]\n",
      "loss: 0.649320  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.7%, Avg loss: 0.654145 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.520622  [    0/60000]\n",
      "loss: 0.694229  [ 6400/60000]\n",
      "loss: 0.425985  [12800/60000]\n",
      "loss: 0.647374  [19200/60000]\n",
      "loss: 0.622226  [25600/60000]\n",
      "loss: 0.602599  [32000/60000]\n",
      "loss: 0.684057  [38400/60000]\n",
      "loss: 0.759643  [44800/60000]\n",
      "loss: 0.709686  [51200/60000]\n",
      "loss: 0.642807  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.649455 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.511679  [    0/60000]\n",
      "loss: 0.684904  [ 6400/60000]\n",
      "loss: 0.421980  [12800/60000]\n",
      "loss: 0.637073  [19200/60000]\n",
      "loss: 0.615178  [25600/60000]\n",
      "loss: 0.589743  [32000/60000]\n",
      "loss: 0.675410  [38400/60000]\n",
      "loss: 0.756649  [44800/60000]\n",
      "loss: 0.706445  [51200/60000]\n",
      "loss: 0.637884  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.646007 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.504870  [    0/60000]\n",
      "loss: 0.677696  [ 6400/60000]\n",
      "loss: 0.419172  [12800/60000]\n",
      "loss: 0.628667  [19200/60000]\n",
      "loss: 0.610082  [25600/60000]\n",
      "loss: 0.579509  [32000/60000]\n",
      "loss: 0.668233  [38400/60000]\n",
      "loss: 0.754199  [44800/60000]\n",
      "loss: 0.703827  [51200/60000]\n",
      "loss: 0.634001  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.643373 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.499550  [    0/60000]\n",
      "loss: 0.671904  [ 6400/60000]\n",
      "loss: 0.417100  [12800/60000]\n",
      "loss: 0.621622  [19200/60000]\n",
      "loss: 0.606270  [25600/60000]\n",
      "loss: 0.571152  [32000/60000]\n",
      "loss: 0.662069  [38400/60000]\n",
      "loss: 0.752223  [44800/60000]\n",
      "loss: 0.701642  [51200/60000]\n",
      "loss: 0.630857  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.0%, Avg loss: 0.641303 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.495308  [    0/60000]\n",
      "loss: 0.667120  [ 6400/60000]\n",
      "loss: 0.415502  [12800/60000]\n",
      "loss: 0.615603  [19200/60000]\n",
      "loss: 0.603326  [25600/60000]\n",
      "loss: 0.564185  [32000/60000]\n",
      "loss: 0.656649  [38400/60000]\n",
      "loss: 0.750646  [44800/60000]\n",
      "loss: 0.699757  [51200/60000]\n",
      "loss: 0.628264  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.639641 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.491866  [    0/60000]\n",
      "loss: 0.663090  [ 6400/60000]\n",
      "loss: 0.414220  [12800/60000]\n",
      "loss: 0.610383  [19200/60000]\n",
      "loss: 0.600983  [25600/60000]\n",
      "loss: 0.558276  [32000/60000]\n",
      "loss: 0.651802  [38400/60000]\n",
      "loss: 0.749396  [44800/60000]\n",
      "loss: 0.698084  [51200/60000]\n",
      "loss: 0.626095  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.1%, Avg loss: 0.638283 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.489032  [    0/60000]\n",
      "loss: 0.659643  [ 6400/60000]\n",
      "loss: 0.413154  [12800/60000]\n",
      "loss: 0.605803  [19200/60000]\n",
      "loss: 0.599062  [25600/60000]\n",
      "loss: 0.553194  [32000/60000]\n",
      "loss: 0.647409  [38400/60000]\n",
      "loss: 0.748411  [44800/60000]\n",
      "loss: 0.696563  [51200/60000]\n",
      "loss: 0.624262  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.2%, Avg loss: 0.637158 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.486667  [    0/60000]\n",
      "loss: 0.656658  [ 6400/60000]\n",
      "loss: 0.412240  [12800/60000]\n",
      "loss: 0.601744  [19200/60000]\n",
      "loss: 0.597443  [25600/60000]\n",
      "loss: 0.548769  [32000/60000]\n",
      "loss: 0.643387  [38400/60000]\n",
      "loss: 0.747640  [44800/60000]\n",
      "loss: 0.695156  [51200/60000]\n",
      "loss: 0.622695  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.3%, Avg loss: 0.636214 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.484670  [    0/60000]\n",
      "loss: 0.654044  [ 6400/60000]\n",
      "loss: 0.411438  [12800/60000]\n",
      "loss: 0.598119  [19200/60000]\n",
      "loss: 0.596043  [25600/60000]\n",
      "loss: 0.544880  [32000/60000]\n",
      "loss: 0.639673  [38400/60000]\n",
      "loss: 0.747041  [44800/60000]\n",
      "loss: 0.693837  [51200/60000]\n",
      "loss: 0.621343  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.635416 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.482965  [    0/60000]\n",
      "loss: 0.651735  [ 6400/60000]\n",
      "loss: 0.410718  [12800/60000]\n",
      "loss: 0.594858  [19200/60000]\n",
      "loss: 0.594806  [25600/60000]\n",
      "loss: 0.541431  [32000/60000]\n",
      "loss: 0.636221  [38400/60000]\n",
      "loss: 0.746581  [44800/60000]\n",
      "loss: 0.692589  [51200/60000]\n",
      "loss: 0.620167  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.634735 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.481495  [    0/60000]\n",
      "loss: 0.649677  [ 6400/60000]\n",
      "loss: 0.410061  [12800/60000]\n",
      "loss: 0.591905  [19200/60000]\n",
      "loss: 0.593690  [25600/60000]\n",
      "loss: 0.538353  [32000/60000]\n",
      "loss: 0.632994  [38400/60000]\n",
      "loss: 0.746232  [44800/60000]\n",
      "loss: 0.691401  [51200/60000]\n",
      "loss: 0.619134  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.634149 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.480216  [    0/60000]\n",
      "loss: 0.647830  [ 6400/60000]\n",
      "loss: 0.409453  [12800/60000]\n",
      "loss: 0.589216  [19200/60000]\n",
      "loss: 0.592669  [25600/60000]\n",
      "loss: 0.535588  [32000/60000]\n",
      "loss: 0.629964  [38400/60000]\n",
      "loss: 0.745975  [44800/60000]\n",
      "loss: 0.690266  [51200/60000]\n",
      "loss: 0.618218  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.5%, Avg loss: 0.633642 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.479093  [    0/60000]\n",
      "loss: 0.646160  [ 6400/60000]\n",
      "loss: 0.408886  [12800/60000]\n",
      "loss: 0.586754  [19200/60000]\n",
      "loss: 0.591722  [25600/60000]\n",
      "loss: 0.533091  [32000/60000]\n",
      "loss: 0.627106  [38400/60000]\n",
      "loss: 0.745791  [44800/60000]\n",
      "loss: 0.689179  [51200/60000]\n",
      "loss: 0.617399  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.633201 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.478100  [    0/60000]\n",
      "loss: 0.644640  [ 6400/60000]\n",
      "loss: 0.408351  [12800/60000]\n",
      "loss: 0.584491  [19200/60000]\n",
      "loss: 0.590833  [25600/60000]\n",
      "loss: 0.530827  [32000/60000]\n",
      "loss: 0.624402  [38400/60000]\n",
      "loss: 0.745668  [44800/60000]\n",
      "loss: 0.688135  [51200/60000]\n",
      "loss: 0.616659  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.632816 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.477214  [    0/60000]\n",
      "loss: 0.643250  [ 6400/60000]\n",
      "loss: 0.407844  [12800/60000]\n",
      "loss: 0.582400  [19200/60000]\n",
      "loss: 0.589993  [25600/60000]\n",
      "loss: 0.528765  [32000/60000]\n",
      "loss: 0.621835  [38400/60000]\n",
      "loss: 0.745595  [44800/60000]\n",
      "loss: 0.687134  [51200/60000]\n",
      "loss: 0.615985  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 76.6%, Avg loss: 0.632478 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, approx_model_2, loss_fn, optimizer)\n",
    "    test_loop(test_loader, approx_model_2, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "arcsin\n",
    "nn with arcsin activation\n",
    "Layer 1: d_in:784, d_out:512 with arcsin activation\n",
    "Layer 2: d_in:512, d_out:10 with no activation\n",
    "\"\"\"\n",
    "class arcsinNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(arcsinNN, self).__init__()\n",
    "        self.Flatten = nn.Flatten() # flatten the input\n",
    "\n",
    "        # Initialize two linear layers with weights from the trained neural network.\n",
    "        self.Linear1 = nn.Linear(28*28,512, bias = False)\n",
    "        self.Linear1.weight = nn.Parameter(torch.clone(approx_model.get_parameter(target='Linear1.weight')))\n",
    "        self.Linear2 = nn.Linear(512,10)\n",
    "        self.Linear2.weight = nn.Parameter(torch.clone(approx_model.get_parameter(target='Linear2.weight')))\n",
    "        self.Linear2.bias = nn.Parameter(torch.clone(approx_model.get_parameter(target='Linear2.bias')))\n",
    "        \n",
    "    def forward(self, x):\n",
    "       \n",
    "        \n",
    "        \n",
    "        x_flat = self.Flatten(x)\n",
    "\n",
    "        x1 = self.Linear1(x_flat)\n",
    "\n",
    "        x2 = torch.tensor(x_flat.T) # [d, n]\n",
    "\n",
    "        W = self.Linear1.weight.T # [d, D]\n",
    "\n",
    "        \n",
    "        x = torch.arcsin(nn.functional.normalize(W, dim = 0).T @ nn.functional.normalize(x2, dim = 0) )\n",
    "        \n",
    "        # second layer\n",
    "        logits = self.Linear2(x.T)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcsin_model = arcsinNN().to(device)\n",
    "print(arcsinmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loop(test_loader,arcsin_model,loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "07315ba070dea1980208edb5f4fd988df0f55a182694c2201b33f2c556751b3c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
